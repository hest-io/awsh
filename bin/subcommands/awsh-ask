#!/usr/bin/env python3
"""
Simple utility to interact with OpenAI. Using this command requires an OpenAI API Key
to be set within the environment (OPENAI_API_KEY). You can read more about getting an
API key as well as how to get started at https://platform.openai.com/docs/introduction/overview

For convenience you can add your OpenAI configuration to your AWSH Global Config by adding a
new section. AWSH Global Config uses INI file format and is loaded from ~/.awsh/config.d/awsh.conf

  [openai]
  api_key = ab-cdhwgelkjlkjkj11134434454523344
  model = gpt-3.5-turbo


Usage:
        awsh-ask [options] [ -i <input>]... [<question>...]
        awsh-ask [options] [ --stdin ] [<question>...]
        awsh-ask [options] ( -f | --forget )
        awsh-ask ( -h | --help )

Options:
        -i, --input=<input>         A file or URL to read from
        --stdin                     Read from stdin
        -f, --forget                Empty the message context and reset using the initial prompt
        -h, --help                  Show this help message and exit
        --debug                     Show more verbose logging
"""

# ruff: noqa: E501

import logging
import os
import pickle
import sys
import base64
import gzip

# Library to provide command line options (e.g. -c classic, -a application, -h help)
import docopt
import openai
from awshutils import check_imports, clean_up
from awshutils.config import get_config_from_file

# Library to benefit from AWSH logging tools
from awshutils.logger import AWSHLog

# Library to read key, value from the options provided in the command line
from future.utils import iteritems

check_imports()

# Logging setup
_log = AWSHLog(__file__)
_log.setLevel(logging.ERROR)
logging.captureWarnings(True)


# Setup OpenAI API credentials and session identity
openai.api_key = os.getenv("OPENAI_API_KEY")
awsh_session_id = os.getenv("AWSH_SESSION_ID")
assistant_cache = []
session_content_buffer = f"/tmp/awsh.assistant_cache.{awsh_session_id}"

initial_context = [
    {
        "role": "user",
        "content": "You are a helpful assistant that provides developers support using AWS APIs and services. If you understand, say OK.",
    },
    {"role": "assistant", "content": "OK"},
]


def encode_to_mime(file_content):
    encoded_content = base64.b64encode(file_content).decode("utf-8")
    mime_encoded_file = f"data:application/octet-stream;base64,{encoded_content}"
    return mime_encoded_file


def encode_files_to_mime(file_names):
    encoded_files = []
    for file_name in file_names:
        with open(file_name, "rb") as file:
            mime_encoded_file = encode_to_mime(file.read())
            encoded_files.append(mime_encoded_file)
    return encoded_files


def encode_files_to_mime_gzip(file_names):
    """
    Take one argument, which is a list of file names and returns a MIME encoded gzipped version of each file
    """
    encoded_files = []
    for file_name in file_names:
        _log.debug(f"Encoding {file_name} for use")
        with open(file_name, "rb") as file:
            file_content = file.read()
            compressed_content = gzip.compress(file_content)
            encoded_content = base64.b64encode(compressed_content).decode("utf-8")
            mime_encoded_file = f"data:application/gzip;base64,{encoded_content}"
            encoded_files.append(mime_encoded_file)
    return encoded_files


def decode_mime_gzip_to_files(encoded_files):
    """
    Take one argument representing a list of MIME-encoded gzipped files and write the uncompressed version of the file contents back to a file with the same name extracted from the MIME-encoded input
    """
    for encoded_file in encoded_files:
        # Extract file name from MIME-encoded input
        file_name = encoded_file.split(";name=")[1]
        _log.debug(f"Decoding contents into {file_name}")
        # Extract base64-encoded gzipped content
        encoded_content = encoded_file.split(",", 1)[1]
        compressed_content = base64.b64decode(encoded_content)

        # Decompress gzipped content
        uncompressed_content = gzip.decompress(compressed_content)

        # Write uncompressed content back to a file
        with open(file_name, "wb") as file:
            file.write(uncompressed_content)


def predict(options):
    question = " ".join(options["<question>"])

    # Load or initialize question cache
    try:
        with open(session_content_buffer, "rb") as f:
            assistant_cache = pickle.load(f)
            _log.debug(f"Session cache contains {assistant_cache}")
    except FileNotFoundError:
        _log.debug("Session cache empty or missing. Creating empty cache")
        assistant_cache = initial_context

    # tokenize the new input sentence
    assistant_cache.append({"role": "user", "content": f"{question}"})

    if options["--input"] is not None:
        _log.debug("User provided file or URL inputs")
        additional_inputs = encode_files_to_mime(options["--input"])
        assistant_cache.extend([{"role": "user", "content": f"{x}"} for x in additional_inputs])

    if options["--stdin"] is True:
        _log.debug("User provided input from STDIN")
        assistant_cache.append({"role": "user", "content": f"{encode_to_mime(sys.stdin.read())}"})

    _log.debug(f"Proceeding with context of {len(assistant_cache)} messages and payload: {question}")

    try:
        completion = openai.ChatCompletion.create(
            model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),  # default: gpt-3.5-turbo - 10x cheaper than davinci, and better. $0.002 per 1k tokens
            messages=assistant_cache,
        )
        # Just the reply:
        reply_content = completion.choices[0].message.content  # .replace('```python', '<pre>').replace('```', '</pre>')

        _log.debug(f"API Consumption was: {completion.usage}")

        print(reply_content)
        assistant_cache.append({"role": "assistant", "content": f"{reply_content}"})

        _log.debug("Writing session cache")
        with open(session_content_buffer, "wb") as f:
            pickle.dump(assistant_cache, f)

        # get pairs of msg["content"] from message history, skipping the pre-prompt:              here.
        response = [
            (assistant_cache[i]["content"], assistant_cache[i + 1]["content"])
            for i in range(2, len(assistant_cache) - 1, 2)
        ]  # convert to tuples of list
        return response

    except openai.error.InvalidRequestError as ex:
        _log.error(f"{ex}")
        return ""


def clear_context():
    assistant_cache = initial_context
    _log.info("Writing empty session cache")
    with open(session_content_buffer, "wb") as f:
        pickle.dump(assistant_cache, f)


# Main function to parse/tabulate the information generated from application and classic functions
def main(options):
    if options["--debug"] is True:
        import logging

        _log.setLevel(logging.DEBUG)

    for key, value in iteritems(options):
        _log.debug("command-line options: {}: {}".format(key, value))

    try:
        if (openai.api_key is not None) and (awsh_session_id is not None):
            if options["--forget"] is True:
                clear_context()
            else:
                predict(options)

        else:
            _log.error("One or more of the required environment variables is not set: OPENAI_API_KEY, AWSH_SESSION_ID")

    except AssertionError as e:
        _log.warn(e)
        clean_up(-1)

    clean_up()


##############################################################################
# Main Script/Loop
##############################################################################

if __name__ == "__main__":
    try:
        options = docopt.docopt(__doc__)
        main(options)

    except docopt.DocoptExit:
        sys.exit(__doc__)
